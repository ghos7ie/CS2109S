{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Regression and Classification\n",
    "\n",
    "**Release Date:** 15 September 2025 1800H\n",
    "\n",
    "**Due Date:** 11 October 2025 2359H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We have learned how to solve a regression problem using linear regression in class.\n",
    "Specifically, we will use the normal equation and gradient descent methods, and apply it to linear regression and polynomial regression models to predict housing prices in Singapore.\n",
    "\n",
    "We have also learned about logistic regression, and how it can be useful as a classification algorithm. We will get some hands-on practice by implementing logistic regression on a Credit Card Fraud Detection dataset. \n",
    "\n",
    "**Required Files**:\n",
    "* ps3.ipynb\n",
    "* housing_data.csv\n",
    "* credit_card.csv\n",
    "\n",
    "**Plagiarism Policy**: Please refer to our [Course Policies](https://canvas.nus.edu.sg/courses/77861/pages/course-policies).\n",
    "\n",
    "**IMPORTANT**: While it is possible to write and run Python code directly in Jupyter notebook, we recommend that you do this Problem set with an IDE using the .py file provided. An IDE will make debugging significantly easier.\n",
    "\n",
    "**Post-Problem Set Survey**:\n",
    "Your feedback is important to us! After completing Problem Set 3, please take a moment to share your thoughts by filling out this [survey](https://coursemology.org/courses/3095/surveys/2720)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation to files\n",
    "\n",
    "**ps3.ipynb**:\n",
    "The template for all your tasks is provided in this file. Some test cases have\n",
    "been provided for you to check the output of your algorithm against the expected result. The tests are **not** comprehensive, and you are\n",
    "encouraged to write your own tests to check for correctness.\n",
    "\n",
    "**housing.csv**:\n",
    "The CSV file contains a dataset with housing data.\n",
    "There are 90 data points. Each data point consists of 3 features:\n",
    "* **floor_area_sqm** - size of the house in square meters\n",
    "* **bedrooms** - number of bedrooms\n",
    "* **schools** - number of primary schools within a 1km radius\n",
    "\n",
    "Our target value is the **asking_price**, which is the price of the housing unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "\n",
    "Similar to PS0, your implementation in the following tasks **should not\n",
    "involve any iteration, including `map` and `filter`, or recursion**. Instead, please work\n",
    "with the operations available in NumPy. Solutions that violate this will be penalised.\n",
    "\n",
    "- You are allowed to use any mathematical functions, but this does not mean that you are allowed to use any NumPy function (there are NumPy functions that aren’t mathematical functions). For example, `np.vectorize` is not allowed since it is iterative. If you are in doubt about which functions are allowed, you should ask in the forum.\n",
    "\n",
    "\n",
    "There is, however, an exception for **Tasks 2.6, 2.8 and 3.5**. In the pseudo-code for the\n",
    "algorithm required, there is an explicit for loop for Tasks 2.6 and 3.5, and a while loop for Task 2.8. Hence, **only for these tasks**, you\n",
    "may use a **single for/while loop** to iterate for the number of epochs required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital imports and setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "###################\n",
    "# Helper function #\n",
    "###################\n",
    "def load_data(filepath):\n",
    "    '''\n",
    "    Load in the given csv filepath as a numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath (string) : path to csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X, y (np.ndarray, np.ndarray) : (n, num_features), (n,) numpy matrices\n",
    "    '''\n",
    "    *X, y = np.genfromtxt(\n",
    "        filepath,\n",
    "        delimiter=',',\n",
    "        skip_header=True,\n",
    "        unpack=True,\n",
    "    ) # default dtype: float\n",
    "    X = np.array(X, dtype=float).T # cast features to int type\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "data_filepath = 'housing_data.csv'\n",
    "X, y = load_data(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining cost functions\n",
    "\n",
    "We need to define cost functions before creating a linear regression model to calculate\n",
    "the error between our prediction and the true values. We will define two cost functions:\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Mean Squared Error (MSE)\n",
    "\n",
    "Write the function `mean_squared_error(y_true, y_pred)` that returns a number representing the mean squared error of the predictions.\n",
    "\n",
    "The formula for Mean Squared Error is as follows:\n",
    "$$ MSE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{2n} \\sum_{i=1}^{n}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the vector with actual values, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $n$ is the number of samples in the\n",
    "training data.\n",
    "\n",
    "**Remark**: The formula here follows the lecture slides for consistency. In definitions and implementations elsewhere, the denominator is usually just $n$ instead of $2n$.\n",
    "\n",
    "**Hint**: Consider using `np.square` or `np.power`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate mean squared error between y_pred and y_true.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true (np.ndarray) : (n, 1) numpy matrix consists of true values\n",
    "    y_pred (np.ndarray) : (n, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean squared error value.\n",
    "    '''\n",
    "    \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "y_true, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_squared_error(y_true, y_pred) in [45.25, 90.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Mean Absolute Error (MAE)\n",
    "\n",
    "Write the function `mean_absolute_error(y_true, y_pred)` that returns a number representing the mean absolute error of the predictions.\n",
    "\n",
    "The formula for Mean Absolute Error is as follows:\n",
    "$$ MAE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{n} \\sum_{i=1}^{n}\\left|\\hat{y}^{(i)} - y^{(i)}\\right| $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the vector with actual values, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $n$ is the number of samples in the\n",
    "training data.\n",
    "\n",
    "**Hint**: Consider using `np.abs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    '''\n",
    "    Calculate mean absolute error between y_pred and y_true.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true (np.ndarray) : (n, 1) numpy matrix consists of true values\n",
    "    y_pred (np.ndarray) : (n, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean absolute error value.\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "y_true, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_absolute_error(y_true, y_pred) == 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression\n",
    "\n",
    "Now we’re ready to create our own linear regression model. We will try to find a linear function, which can be written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_d x_d $$\n",
    "\n",
    "where $y$ is the target value, $x_1, x_2, \\dots, x_d$ are feature values, and $w_0, w_1, \\dots, w_d$ are parameters, where $d$ is the number of features. $w_0$ is meant to represent the bias term, while $w_1, \\dots, w_d$ are the feature weights.\n",
    "\n",
    "**Bias term**\n",
    "\n",
    "The bias term ($w_0$) is useful in capturing an inherent offset of the target values from the origin, i.e. they have some non-zero \"default\" or \"starting\" value. The bias term accounts for this default value in our model. Without a bias term (or bias = 0), our regression lines will pass through the origin, which might not be appropriate for the data in question.\n",
    "\n",
    "Consider the scatter plot below. The blue line is the best fitting line without a bias term, while the red line includes a non-zero bias. Since the blue line starts at the origin, it is unable to capture the offset of the points. In contrast, the red line starts higher (at around 5), and hence is better able to approximate the data.\n",
    "\n",
    "&nbsp;\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/bias_scatter.png\" alt=\"bias vs no bias\" width=\"40%\">\n",
    "<figcaption style=\"text-align:center\">Figure 1: Example of models with bias vs without bias.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Usually, we have to explicitly add a bias term into our data when building our models. In the following tasks, you'll explore how to do so and how this choice can affect the accuracy of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Adding a bias column\n",
    "\n",
    "In the lecture, we learned that adding a bias allows our linear model to be more\n",
    "flexible. Write the function `add_bias_column(X)` that takes a NumPy matrix `X` and returns\n",
    "a new matrix with an additional column. The additional column should have all of its\n",
    "elements set to 1 and is located at the first column of the matrix.\n",
    "\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/add_bias.jpeg\" alt=\"adding bias\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 2: Example of a matrix before and after adding a bias column.</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Note**: Your function should work for all kinds of matrix shapes.\n",
    "\n",
    "**Hint**: Consider using `np.hstack` to add the bias column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_column(X):\n",
    "    '''\n",
    "    Create a bias column and combine it with X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n, d) numpy matrix representing a feature matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        new_X (np.ndarray):\n",
    "            A (n, d + 1) numpy matrix with the first column consisting of all 1s\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "without_bias = np.array([[1, 2], [3, 4]])\n",
    "expected = np.array([[1, 1, 2], [1, 3, 4]])\n",
    "\n",
    "assert np.array_equal(add_bias_column(without_bias), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "\n",
    "We will first use the normal equation to obtain $w_0, w_1, \\dots, w_d$. The solution to the normal equation is as\n",
    "follows:\n",
    "\n",
    "$$ \\begin{pmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d \\end{pmatrix} = (X^TX)^{-1}X^T \\boldsymbol{y} $$\n",
    "\n",
    "where $X$ is the (augmented for bias) feature matrix and $\\boldsymbol{y}$ is the vector of target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Find optimal bias and weights using normal equation\n",
    "\n",
    "Write the function `get_bias_and_weight(X, y, include_bias)` that returns $w_0$ (bias) and\n",
    "$w_1, w_2, \\dots, w_d$ (weights) that will lead to best fitting line, using the **normal equation** method. \n",
    "\n",
    "The `include_bias` argument is used to specify if the model includes a bias term, i.e. has a non-zero bias term. Hence, the function should return $w_0 = 0$ if it is set to `false`. The function should return $w_1, \\dots, w_d$ as a NumPy matrix with shape $(d, 1)$, where $d$ is the number of features (excluding the bias column).\n",
    "\n",
    "**Note**: Make use of the `add_bias_column` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n",
    "\n",
    "**Hint**: Consider using `numpy.linalg.inv` for the matrix inverse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_and_weight(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate bias and weights that give the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, d) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (n, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            If include_bias = True, return the bias constant. Else,\n",
    "            return 0\n",
    "        weights (np.ndarray):\n",
    "            A (d, 1) numpy matrix representing the weight(s).\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "public_X, public_y = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "test_1 = (round(get_bias_and_weight(public_X, public_y)[0], 5) == 3)\n",
    "test_2 = np.array_equal(np.round(get_bias_and_weight(public_X, public_y)[1], 1), np.array([[1.0], [0.0]]))\n",
    "test_3 = np.array_equal(np.round(get_bias_and_weight(public_X, public_y, False)[1], 2), np.round(np.array([[0.49], [1.20]]), 2))\n",
    "\n",
    "assert test_1 and test_2 and test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_linear_regression(X, y, include_bias)` that returns `y_pred`,\n",
    "a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: Make use of the `get_bias_and_weight` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_linear_regression(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, d) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (n, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        y_pred (np.ndarray):\n",
    "            A (n, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "  \n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_X, test_y = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "assert round(mean_squared_error(test_y, get_prediction_linear_regression(test_X, test_y)), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a linear regression model with **floor_area_sqm** as the only\n",
    "feature and **asking_price** as the target value. Plot your prediction line using the code\n",
    "snippet below. It should look similar to Figure 3.\n",
    "\n",
    "&nbsp;\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/linear_reg.png\" alt=\"regression plot\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 3: Example of linear regression using <b>floor_area_sqm</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = X[:, 0].reshape((-1, 1))\n",
    "predicted = get_prediction_linear_regression(area, y)\n",
    "plt.scatter(area, y)\n",
    "plt.plot(area, predicted, color = 'r')\n",
    "plt.xlabel(\"Size in square meter\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "\n",
    "In real-world data, a straight line might not fit the data perfectly. Consider the relation between **schools** and **asking_price**.\n",
    "\n",
    "&nbsp;\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/school_price_rel.png\" alt=\"school price relation\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 4: Schools - Price Relationship.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Houses with 0 schools nearby tend to be cheaper than houses with 1 school nearby. However, as the number of schools increases, the prices decrease. If we try a linear regression on the data, we obtain a straight line. Notice how we lose the detail that houses with 0 schools are actually cheaper than houses with 1 school nearby. A polynomial function can better capture this relationship.\n",
    "\n",
    "&nbsp;\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<div>\n",
    "<img src=\"./images/school_price_rel_linearfit.png\" alt=\"school price relation linear fit\" width=\"45%\">\n",
    "<img src=\"./images/school_price_rel_cubicfit.png\" alt=\"school price relation cubic fit\" width=\"45%\">\n",
    "</div>\n",
    "<figcaption style=\"text-align:center\">Figure 5: Schools - Price Relationship With Linear and Cubic fit.</figcaption>\n",
    "</figure>\n",
    "\n",
    "A polynomial function is written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x + w_2 x^2 + ... + w_d x^d $$\n",
    "\n",
    "where $y$ is the target value, $x$ is a (*single*) feature value, and $d$ is the degree of the polynomial. $w_0$ is the bias term and $w_1, \\dots, w_d$ are the feature weights. \n",
    "\n",
    "Notice how if we set $x_1 = x, x_2 = x^2, \\dots, x_d = x^d$, then the polynomial function is simply linear regression with $d$ features:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_d x_d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 : Create Polynomial Matrix\n",
    "\n",
    "Write the function `create_polynomial_matrix(X, power)` that takes a $(n, 1)$-matrix and an\n",
    "integer, and returns a polynomial matrix with shape $(n, power)$.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{array}\\right]\n",
    "\\xrightarrow[]{\\text{create\\_polynomial\\_matrix(3)}}\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 1^2 & 1^3\\\\ \n",
    "2 & 2^2 & 2^3\\\\\n",
    "3 & 3^2 & 3^3\n",
    "\\end{array}\\right]\n",
    "\\rightarrow\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 1 & 1\\\\ \n",
    "2 & 4 & 8\\\\\n",
    "3 & 9 & 27\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "**Hint**: Consider using `np.tile`/`np.repeat` together with `np.cumprod`/`np.power`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_matrix(X, power = 2):\n",
    "    '''\n",
    "    Create a polynomial matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (n, 1) numpy matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (n, power) numpy matrix where the i-th column denotes\n",
    "            X raised to the power of i.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "vector = np.array([[1], [2], [3]])\n",
    "poly_matrix = np.array([[1, 1, 1], [2, 4, 8], [3, 9, 27]])\n",
    "\n",
    "assert np.array_equal(create_polynomial_matrix(vector, 3), poly_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_poly_regression(X, y, power, include_bias)` that returns\n",
    "`y_pred`, a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: Make use of the `create_polynomial_matrix` and `get_prediction_linear_regression` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_poly_regression(X, y, power = 2, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best polynomial line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, 1) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (n, 1) numpy matrix representing target values\n",
    "    power (int) : Specify the degree of the polynomial\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (n, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_X, test_y = np.arange(3).reshape((-1, 1)), np.arange(4, 7).reshape((-1, 1))\n",
    "pred_y = get_prediction_poly_regression(test_X, test_y, 2)\n",
    "\n",
    "assert round(mean_squared_error(test_y, pred_y), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a polynomial regression model, using `power = 3` and `include_bias = True`, with **schools** as the only feature and **asking_price** as the target value. Plot your prediction line using the code snippet below. It should look similar to Figure 6.\n",
    "\n",
    "&nbsp;\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/poly_reg.png\" alt=\"polynomial regression\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\">Figure 6: Example of polynomial regression using <b>schools</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "schools = X[:, 2].reshape((-1, 1))\n",
    "predicted = get_prediction_poly_regression(schools, y, 3)\n",
    "plt.scatter(schools, y)\n",
    "plt.scatter(schools, predicted, color = 'r', s = 100)\n",
    "plt.xlabel(\"Number of schools within 1km\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "We will now learn to use gradient descent to approximate $\\boldsymbol{w} = w_0, w_1, \\dots, w_d$.\n",
    "\n",
    "*Gradient descent*<sup>&#x2020;</sup> is an algorithm that minimizes the cost function by iteratively trying to\n",
    "find the best parameters. In linear regression, we will try to minimize the Mean Squared\n",
    "Error. The outline of the algorithm is as follows:\n",
    "    \n",
    "* Start with some $\\boldsymbol{w} = (w_0, \\dots, w_d)$\n",
    "* Keep changing $w_0,\\dots, w_d$ to minimize $J(\\boldsymbol{w})$, where $J$ is our cost function\n",
    "\n",
    "In this problem set, we will initially set $w_0, w_1, \\dots, w_d$ to all be 0s. Then, we will set a\n",
    "learning rate $\\gamma$ that will affect the rate of change of $w_0, \\dots, w_d$. Lastly, we will set\n",
    "$T$ to specify the number of epochs of gradient descent we want to run.\n",
    "\n",
    "The pseudo-code of Gradient Descent for linear regression is defined in Algorithm 1.\n",
    "\n",
    "**Note**: In the following gradient descent-related tasks, calculate the value of the loss function *after* updating the bias and weights.\n",
    "\n",
    "<sup>&#x2020;</sup> *The Gradient Descent algorithm is not limited to the linear regression model – it is a general optimisation technique and is also used in many other machine learning models such as Neural Networks.*\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"./images/mse_plot.png\" alt=\"gradient descent\" style=\"width: 45%; margin-right: 10px;\">\n",
    "    <img src=\"./images/grad_desc_algorithm.png\" alt=\"gradient descent\" style=\"width: 45%;\">\n",
    "</div>\n",
    "<figcaption style=\"text-align:center; margin-top: 10px;\">Figure 7: Gradient descent tries to find parameters that lead to the lowest MSE.</figcaption>\n",
    "\n",
    "For MSE, The partial derivative $\\frac{\\partial J_{MSE}(\\boldsymbol w)}{\\partial w_j}$ with $n$ training samples can be derived as: \n",
    "$$\n",
    "\\frac{\\partial J_{MSE}(\\boldsymbol{w})}{\\partial w_j} = \\frac 1n\\sum^n_{i=1}(h_w(x^{(i)})-y^{(i)})\\cdot x_j^{(i)} \n",
    "$$\n",
    "where $h_w$ is our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Gradient Descent on multiple features\n",
    "\n",
    "Write the function `gradient_descent_multi_variable(X, y, lr, number_of_epochs)` that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_d$ - $(d, 1)$ NumPy matrix, where each element denotes the weights of a certain feature\n",
    "* $loss$ - a list that contains the MSE scores calculated during the gradient descent process.\n",
    "\n",
    "**Note**: Make use of the `mean_squared_error` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250):\n",
    "    '''\n",
    "    Bias and weight that gives the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, d) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (n, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    number_of_epochs (int) : Number of gradient descent epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (d, 1) numpy matrix that specifies the weights.\n",
    "        loss (list):\n",
    "            A list where the i-th element denotes the MSE score at i-th epoch.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    loss = []\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    pred = X @ weights + bias\n",
    "    for _ in range(number_of_epochs):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "    \n",
    "    return bias, weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "_, _, loss = gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_epochs = 250)\n",
    "loss_initial = loss[0]\n",
    "loss_final = loss[-1]\n",
    "\n",
    "assert loss_initial > loss_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7 Feature Scaling\n",
    "\n",
    "As we create a higher degree polynomial matrix, each column will have a larger scale\n",
    "than the previous one, which can lead to poor performance for gradient descent. This also applies to datasets with multiple features where the features are already of different scales. Here\n",
    "is where feature scaling plays an important role. \n",
    "\n",
    "Write the function `feature_scaling(X)`\n",
    "that takes a NumPy matrix `X` and returns a matrix with standardized columns.\n",
    "\n",
    "**Note**: The normalization occurs on the column level ($i$-th column is normalized by the\n",
    "mean and standard deviation of the $i$-th column). That is,\n",
    "\n",
    "$$\n",
    "\\text{If} \\quad \\boldsymbol{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_N \\end{pmatrix} \\\\\n",
    "\\\\\n",
    "\\boldsymbol{v}_{norm} = \\frac{\\boldsymbol{v} - \\boldsymbol{\\hat{v}}}{\\sigma_{v}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{v}$ is a vector of $n$ elements, $\\boldsymbol{\\hat{v}}$ is its mean, and $\\sigma_{v}$ is its standard deviation.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "1 & 133\\\\\n",
    "4 & 700\\\\\n",
    "5 & 133\\\\\n",
    "8 & 700\n",
    "\\end{array}\\right]\n",
    "\\xrightarrow[]{\\text{feature\\_scaling}}\n",
    "\\left[\\begin{array}{cc} \n",
    "-1.4 & -1\\\\\n",
    "-0.2 & 1\\\\\n",
    "0.2 & -1\\\\\n",
    "1.4 & 1\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "\n",
    "Focusing on the first feature:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v} = \\left(\\begin{array}{cc}\n",
    "1\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "8\n",
    "\\end{array}\\right), \\boldsymbol{\\hat{v}} = 4.5, \\sigma_{v} = 2.5$$\n",
    "\n",
    "$$\\boldsymbol{v}_{norm} = \\frac{\\boldsymbol{v} - 4.5}{2.5} = \\left(\\begin{array}{cc}\n",
    "-1.4\\\\\n",
    "-0.2\\\\\n",
    "0.2\\\\\n",
    "1.4\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "**Hint**: Consider using `np.mean()` and `np.std()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    '''\n",
    "    Standardize each feature column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, d) numpy matrix representing feature matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (n, d) numpy matrix where each column has been standardized.\n",
    "    '''\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "public_X = np.array([[1, 133], [4, 700], [5, 133], [8, 700]])\n",
    "expected = np.array([[-1.4, -1], [-0.2, 1], [0.2, -1], [1.4, 1]])\n",
    "\n",
    "assert np.array_equal(feature_scaling(public_X), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.8: Finding the number of epochs for gradient descent to converge\n",
    "\n",
    "Fill in the function `find_number_of_epochs(X, y, lr, delta_loss)` that that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_d$ - $(d, 1)$ NumPy matrix, where each element denotes the weights of a certain feature\n",
    "* $num\\_of\\_epochs$ - a number representing the number of epochs performed to reach convergence\n",
    "\n",
    "A single epoch is defined as performing the gradient descent *once* and calculating the loss. The loss calculation and gradient descent should be performed using MSE.\n",
    "\n",
    "The definition of convergence is as follows:\n",
    "\n",
    "$$ |J_{t-1} - J_{t}| < delta\\_loss $$\n",
    "\n",
    "where $J_{t-1}$ is loss at timestep $t-1$ (previous timestep), $J_{t}$ is loss at timestep $t$ (current timestep), and $delta\\_loss$ is the termination criterion.\n",
    "\n",
    "**Note**: Make use of the `mean_squared_error` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_of_epochs(X, y, lr, delta_loss):\n",
    "    '''\n",
    "    Do gradient descent until convergence and return number of epochs\n",
    "    required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (n, d) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (n, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    delta_loss (float) : Termination criterion\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (d, 1) numpy matrix that specifies the weights.\n",
    "        num_of_epochs (int):\n",
    "            Number of epochs to reach convergence.\n",
    "        current_loss (float):\n",
    "            The loss value obtained after convergence.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    num_of_epochs = 0\n",
    "    previous_loss = 1e14\n",
    "    current_loss = -1e14\n",
    "\n",
    "    n = X.shape[0]\n",
    "    pred = X @ weights + bias\n",
    "    current_loss = mean_squared_error(y, pred)\n",
    "    while abs(previous_loss - current_loss) >= delta_loss:\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \"\"\" YOUR CODE END HERE \"\"\"\n",
    "    \n",
    "    return bias, weights, num_of_epochs, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "poly_X = create_polynomial_matrix(X[:, 2].reshape((-1, 1)), 3)\n",
    "_, _, num_of_epochs, _ = find_number_of_epochs(poly_X, y, 1e-5, 1e7)\n",
    "\n",
    "assert num_of_epochs > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.9: Which algorithm should we use for Linear Regression?\n",
    "\n",
    "Compare the pros and cons of using normal equation and gradient descent for linear regression. Specifically:\n",
    "\n",
    "- Compare the speed of the two algorithms on data with many features. \n",
    "- Compare the quality of the solutions obtained by the two algorithms. (i.e. how close to the optimal solution are the solutions obtained by the algorithms)\n",
    "- Compare whether feature scaling is necessary for each algorithm to perform well.\n",
    "\n",
    "Finally, select the algorithm you think is more suitable for this dataset and explain why you chose it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.10: Analyze the effects of feature scaling on Gradient Descent\n",
    "\n",
    "In this task, we will examine the influence of feature scaling on the efficiency of gradient descent algorithms. Specifically, we utilize a degree 3 polynomial feature matrix derived from a dataset of schools.\n",
    "\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/loss_vs_epochs.png\" alt=\"loss against epochs with and without normalization\" width=\"90%\">\n",
    "<figcaption style=\"text-align:center\">Figure 8: Loss against epoch count, for gradient descent without and with normalization</figcaption>\n",
    "</figure>\n",
    "\n",
    "Focus on two key aspects:\n",
    "\n",
    "1. Convergence analysis\n",
    "    - Observe the number of epochs required to achieve convergence for both the non-normalized (original) and normalized feature matrices.  \n",
    "    This comparison should be conducted across various learning rates.\n",
    "2. Loss visualization\n",
    "    - Analyze the loss values corresponding to both the non-normalized and normalized matrices as a function of epoch count, again considering different learning rates.\n",
    "\n",
    "\n",
    "**Draw 2 observations or conclusions** from the above figures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Fraudulent credit card transaction is a common phenomenon in many parts of the world and can lead to potentially large amounts of losses for both companies and customers. Therefore, we hope to help credit card companies recognize those fraudulent transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "We are given a dataset that contains transactions made by credit cards holders in `credit_card.csv`. If we think about what type of data might be included in the input variables under the given context, we might realize that those input variables are likely to include word descriptions, such as shop name or locations. In this problem set, we don't need to worry about language processing as the data are pre-processed to contain only numeric values.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Take a look at the columns in the dataset `credit_card.csv`. We have V1-V20, 'Amount', and 'Time' as input features, and 'Class' as output which takes the value 1 if it's fraud and 0 otherwise. This dataset presents 492 frauds out of 284,807 transactions. That means, there are 284,808 rows (including the header) in the csv file.\n",
    "\n",
    "We will use this dataset to implement logistic regression using stochastic gradient descent for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Read credit card data into a Pandas dataframe for large tests\n",
    "\n",
    "dirname = os.getcwd()\n",
    "credit_card_data_filepath = os.path.join(dirname, 'credit_card.csv')\n",
    "\n",
    "credit_df = pd.read_csv(credit_card_data_filepath)\n",
    "X_task5 = credit_df.values[:, :-1]\n",
    "y_task5 = credit_df.values[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is an open source data analysis and manipulation tool in Python. In this problem set, we read the CSV into a [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to provide some nifty methods that makes it easier for us to handle large amounts of data. You can think of a dataframe as a large table that stores our dataset in a neat and optimized manner, making it fast for retrieval and manipulation and data. Using Pandas, we can quickly gain an overview of the type and values of the data stored, distributions of values within the dataset, and even ways to perform sampling.\n",
    "\n",
    "In the new few sections, we will explore some basic functions of Pandas to help us get started. You do not need to submit any codes for this section, and can simply run the cells to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n",
    "\n",
    "Using the [`head`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method on the dataframe, we can get an overview of the data. By default, the method returns the first 5 entries in the dataframe.\n",
    "\n",
    "Next, we can inspect the [value counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) of the 'Class' property in the dataframe to know the number of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_df.head())\n",
    "\n",
    "# Inspect the number of fraudulent and non-fraudulent transactions.\n",
    "print(credit_df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and selecting data\n",
    "\n",
    "Similar to NumPy, we can also index and select data on Pandas dataframes.\n",
    "\n",
    "For example, we can [select columns in the dataframe by their labels](https://pandas.pydata.org/docs/user_guide/indexing.html#basics). We use `'Class'` to index the 'Class' column in the credit dataframe, or use integer indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'Class' column in the credit dataframe\n",
    "print(credit_df['Class'])\n",
    "\n",
    "# Obtain the first 2 rows\n",
    "print(credit_df[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also [select columns in the dataframe that fulfils some condition](https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing). In the example below, `credit_df['Class'] == 0` returns a [Pandas Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) of length 284807, which is the size of our dataset. It contains the value `True` if and only if the `Class` value for the particular entry is of value 0, and `False` otherwise. We can use this Boolean series to index the credit dataframe to return the rows where the `Class` field is 0. Does this remind you of how NumPy arrays operate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_df['Class'] == 0)\n",
    "\n",
    "# Obtain the credit dataframe where the 'Class' field is 0\n",
    "print(credit_df[credit_df['Class'] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also [concatenate](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) Pandas series or dataframes together! In this example, we explored how we can concat the first 2 rows and last 2 rows of the dataframe. Similar to NumPy, you would also need to specify the axis for concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([credit_df[:2], credit_df[-2:]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Problem with imbalanced data\n",
    "\n",
    "Investigate the dataset by comparing the number of fradulent and non-fradulent transactions. Describe in one or two sentences what problem we might encounter if we directly use the given dataset without processing it. How can we address this issue? \n",
    "\n",
    "Hint: consider the `Class` column, and think about how a high accuracy prediction can be achieved in a wrong way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data\n",
    "\n",
    "Before we can start training our models, we need to **randomly** partition our dataset into training data and testing data. \n",
    "\n",
    "Remember that we are trying to make a model that can predict fraudulence of data points that the model has never seen. It would be unwise to measure the accuracy of the model using the same data it trained on, as that does not give you an idea of how well the model will work on unseen data. Hence, we will have to train the model on a training set and evaluate its effectiveness (e.g. using metrics such as accuracy, F1 score etc.) on a separate test set. The [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) library provides methods to perform a [train-test data split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Correspondingly, if you intend to do more with your model, for example, fine-tune it by adjust various hyperparameters or compare it with other models, you are advised to use separate development or validation sets respectively (which are disjoint from the training and test sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "Recall the method you learned and applied in Part 2 of the problem set. That is an implementation of batch gradient descent for linear regression. In batch gradient descent, **all** the training data (entire batch) is taken into consideration in a single step. \n",
    "\n",
    "The cost function is different for logistic regression, and thus the weight update also requires a different function.  \n",
    "\n",
    "#### Termination Condition\n",
    "\n",
    "In Task 2.6, we run gradient descent for a fixed number of iterations. However, we can also terminate early once we have reached convergence. Here are some possible criteria for terminating gradient descent:\n",
    "* Stop when error change is small and/or\n",
    "* Stop when error is small\n",
    "* Stop when maximum number of iterations is reached\n",
    "\n",
    "For the remaining tasks, you can assume that the bias column has been added for all input `X`, and you don't need to manually add the bias column again in your code. \n",
    "\n",
    "We will not be implementing the full batch gradient descent algorithm as you have already done so in Task 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Cost function\n",
    "\n",
    "Recall that for logistic regression, we use the binary cross-entropy function:\n",
    "\n",
    "$$BCE(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat y)$$\n",
    "\n",
    "In this task, you need to implement `cost_function` using $BCE$. This function takes `X`, `y`, and `weight_vector` $\\mathbf{w}$ as arguments, and returns the error $BCE$. Note that for this task, the $BCE$ should account for **all** the training data.\n",
    "\n",
    "Here, we are using the $\\log$ function and we need to handle the case of computing $\\log(0)$. There are many ways to handle this. In this task, we will handle $\\log(0)$ by using the machine epsilon `np.finfo(np.float64).eps`, and use the trick $\\log(x + eps)$ which allows $x$ to be $0$. If $x$ was any other value, $\\log(x + eps)$ would be very close to $\\log(x)$. This helps to improve numerical stability in computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X: np.ndarray, y: np.ndarray, weight_vector: np.ndarray):\n",
    "    '''\n",
    "    Bianry cross entropy error for logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (n, d) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (n,) training dataset (corresponding targets).\n",
    "    weight_vector: np.ndarray\n",
    "        (d,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BCE cost\n",
    "    '''\n",
    "    \n",
    "    # Machine epsilon for numpy `float64` type\n",
    "    eps = np.finfo(np.float64).eps\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1], [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([0.002, 0.1220])\n",
    "\n",
    "assert np.round(cost_function(X1, y1, w1), 5) == np.round(1.29333, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Weight update for batch gradient descent\n",
    "\n",
    "In this task, you need to implement `weight_update`. This function takes `X`, `y`, `gamma`, and a `weight_vector` as arguments, and output the new weight vector based on all values of `X` and `y`. Each call to the function should make one update on the weight vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(X: np.ndarray, y: np.ndarray, gamma: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (n, d) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (n,) training dataset (corresponding targets).\n",
    "    gamma: np.float64\n",
    "        logistic regression learning rate.\n",
    "    weight_vector: np.ndarray\n",
    "        (d,) weight parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New weight vector after one round of update.\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],[111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([2.2000, 12.20000])\n",
    "a1 = 1e-5\n",
    "nw1 = np.array([2.199,12.2])\n",
    "\n",
    "assert np.array_equal(np.round(weight_update(X1, y1, a1, w1), 3), nw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Logistic regression classification\n",
    "\n",
    "Remember that logistic regression is used for classification even though the function gives a probability output. In this task, you classify each element in `X`, given `weight_vector` using `prob_threshold` as the threshold, and output the classification result as an `np.ndarray`.\n",
    "\n",
    "If the probability predicted by the `weight_vector` exceeds the `prob_threshold`, we should classify it as fraud (`y = 1`). Otherwise, we should classify it as legitimate (`y = 0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classification(X: np.ndarray, weight_vector: np.ndarray, prob_threshold: np.float64=0.5):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (n, d) training dataset (features).\n",
    "    weight_vector: np.ndarray\n",
    "        (d,) weight parameters.\n",
    "    prob_threshold: np.float64\n",
    "        the threshold for a prediction to be considered fraudulent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (n,) np.ndarray\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([-0.000002, 0.000003])\n",
    "expected1 = np.transpose([0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "result1 = logistic_regression_classification(X1, w1)\n",
    "\n",
    "assert result1.shape == expected1.shape and (result1 == expected1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent\n",
    "\n",
    "What happens to batch gradient descent if the dataset is very large? The runtime of the algorithm increases drastically. \n",
    "\n",
    "In Stochastic Gradient Descent (SGD), we consider only the error on a single data point $(x^{(*)}, y^{(*)})$ at a time. \n",
    "\n",
    "During the training loop, we will randomly sample an instance in the dataset. This instance will be used to update the weights of the model \n",
    "\n",
    "$$ w_j \\leftarrow w_j - \\gamma \\frac{\\partial J_{stoch}(\\mathbf{w})}{\\partial w_j} $$\n",
    "\n",
    "where $\\gamma$ is the learning rate, simultaneously for all $j$.\n",
    "\n",
    "Note that SGD does not necessarily decrease the batch loss in each iteration! However, on average, the loss will still decrease. Since we work on only a single data point at any time, SGD provides the advantage of a much smaller memory requirement, and faster computation time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Logistic regression using stochastic gradient descent\n",
    "\n",
    "In this task, you need to implement logistic regression using stochastic gradient descent. This function takes `X_train`, `y_train`, `max_num_iterations`, `threshold`, `gamma`, and `seed` as arguments, and output the final `weight_vector` you obtained. \n",
    "\n",
    "For stochastic gradient descent, an epoch refers to one pass through **one randomly selected data point**. Hence, one iteration of gradient descent, which updates the weight vector based on the cost function computed on the randomly selected data point, corresponds to one epoch. Stochastic gradient descent is terminated when the number of update rounds exceeds the maximum number of epochs (`max_num_iterations`) given, or when the computed error decreases to or below the threshold value (`threshold`).\n",
    "\n",
    "For this task, you should use `np.random.choice(..., size=1)` to randomly choose the training data for each iteration. You should also use the `seed` argument with `np.random.seed(seed)` to set the random seed for reproducibility.\n",
    "\n",
    "For your convenience, you may define your own `weight_update_stochastic` method, which does the weight update for one step in stochastic gradient descent, inside the `logistic_regression_stochastic_gradient_descent` method.\n",
    "\n",
    "**Note**: Make use of the `cost_function` function for this task. (The function has been predefined for you in Coursemology. Do not redefine the function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_iterations: int=250, threshold: np.float64=0.05, gamma: np.float64=1e-5, seed: int=43) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write a terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (n, d) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (n,) training dataset (corresponding targets).\n",
    "    max_num_iterations: int\n",
    "        this should be one of the terminating conditions. \n",
    "        The gradient descent step should happen at most max_num_iterations times.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    gamma: np.float64\n",
    "        logistic regression learning rate.\n",
    "    seed: int\n",
    "        seed for random number generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (d,) weight parameters\n",
    "    '''\n",
    "\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \"\"\" YOUR CODE END HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],[111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = cost_function(X1, y1, np.transpose(np.zeros(X1.shape[1])))\n",
    "\n",
    "assert cost_function(X1, y1, logistic_regression_stochastic_gradient_descent(X1, y1)) < expected1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to verify your implementation of `logistic_regression_stochastic_gradient_descent` by running the code below. \n",
    "\n",
    "If you had implemented every step correctly, you should expect an accuracy of around ~80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_neg = credit_df[credit_df[\"Class\"] == 0]\n",
    "df_pos = credit_df[credit_df[\"Class\"] == 1]\n",
    "df_neg = resample(df_neg, replace=True, n_samples=500, random_state=42)\n",
    "df_pos = resample(df_pos, replace=True, n_samples=500, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_neg, df_pos])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df_balanced.drop(columns=[\"Class\"]).to_numpy()\n",
    "y = df_balanced[\"Class\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = add_bias_column(X_train)\n",
    "X_test = add_bias_column(X_test)\n",
    "\n",
    "weights = logistic_regression_stochastic_gradient_descent(X_train, y_train)\n",
    "\n",
    "y_pred_train = logistic_regression_classification(X_train, weights)\n",
    "y_pred_test = logistic_regression_classification(X_test, weights)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.5f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.6: Stochastic gradient descent vs batch gradient descent\n",
    "\n",
    "Recall that it is ideal for gradient descent to run until convergence, but convergence might be hard to obtain.\n",
    "\n",
    "In this task, we will explore the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent and stochastic gradient descent using a sample of the `credit_card` dataset.\n",
    "\n",
    "Upon measuring the runtime of batch and stochastic gradient descent for different epoch counts, we get the following graphs:\n",
    "\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<div>\n",
    "<img src=\"./images/batch_vs_stochastic_update_rounds.png\" alt=\"CE loss against epoch count\" width=\"45%\">\n",
    "<img src=\"./images/batch_vs_stochastic_runtime.png\" alt=\"CE loss against runtime\" width=\"45%\">\n",
    "</div>\n",
    "<figcaption style=\"text-align:center\">Figure 9: cross entropy loss vs number of update rounds and runtime for batch and stochastic gradient descent.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Do note that the runtime is the cumulative runtime of the entire run of gradient descent, not the runtime per individual update round. \n",
    "\n",
    "With reference to Figure 9, what do you observe about the relationship between the cross entropy loss vs the number of update rounds and/or the runtime it takes to run the update for both batch gradient descent and stochastic gradient descent?\n",
    "\n",
    "Hint: You may/may not angle your answer in terms of explaining the effect of the size of the dataset, whether the algorithm will be stuck in a local minima and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Support Vector Machine (Optional)\n",
    "\n",
    "Support vector machine (SVM) will be covered in lecture 7. You may attempt to follow the guide here, or attempt after the lecture. There is no need to submit anything in Coursemology. \n",
    "\n",
    "Here is a quick recap of what SVM is about: We are given a training dataset of $n$ points of the form $(x^{(i)}, y^{(i)})$, where the $y^{(i)}$ are labels, each indicating the class to which the point $x^{(i)}$ belongs. Typically, the SVM is formulated with $y^{(i)}$ being either 1 or -1, however the following discussion uses equivalently $y^{(i)}$ being either 1 or 0. Be careful that some examples in this problem use data with 1,0 and some examples use data with 1,-1. \n",
    "\n",
    "Each $x^{(i)}$ $n$-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points $x^{(i)}$ for which $y_{i}=1$ from the group of points for which $y_{i}=0$, which is defined so that the distance between the hyperplane and the nearest point $x^{(i)}$ from either group is maximized.\n",
    "\n",
    "<figure style=\"display: flex; flex-direction:column; justify-content: center; align-items: center\">\n",
    "<img src=\"./images/svm.png\" alt=\"visualisation of support vector machine\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 10: Visualisation of support vector machine.</figcaption>\n",
    "</figure>\n",
    "\n",
    "How can we implement this in code? Thankfully, with modern machine learning libraries, much of the tedious work of solving this has been done. As machine learning practitioners, one popular library that we commonly use is [scikit-learn](https://scikit-learn.org/), which is built on top of NumPy and provides simple and efficient tools to implement common machine learning algorithms.\n",
    "- Note: sklearn implements a variant of SVM called [_soft-margin SVM_](https://en.wikipedia.org/wiki/Support_vector_machine#:~:text=support%20vectors.-,Soft%2Dmargin,-%5Bedit%5D) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM vs Gaussian Kernel SVM\n",
    "\n",
    "The code for `linear_svm` and `gaussian_kernel_svm` has been given to you. These functions take in `X` and `y` (the entire dataset), do a train test data split with `test_size` of 0.3 and `random_state` of 42, create an instance of a Linear/Gaussian kernel SVM classifier using the default parameters, train the classifier, and output the predictions as well as the accuracy score. Run the code cells below (you may modify them if you wish).\n",
    "\n",
    "Based on your observations, when using support vector machines, how do you think we should choose between linear kernel vs Gaussian kernel? In other words, which kernel is better in what cases? \n",
    "\n",
    "- Linear kernel is preferable when the data is linearly separable, otherwise Gaussian kernel would perform better. \n",
    "\n",
    "- Moreover, linear kernel is more suitable when number of features is a lot larger than the number of samples in the training set. In the opposite situation, Gaussian kernel is more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm(X: np.ndarray, y: np.ndarray):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    clf_predictions = clf.predict(X_test)\n",
    "\n",
    "    return clf_predictions, clf.score(X_test, y_test) * 100\n",
    "\n",
    "\n",
    "# small data\n",
    "# Do note that y values for data1 are either 0 or 1 for this half of the task, but typically are \n",
    "#   either -1 or 1 for SVMs. You do not have to change this data for this task.\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "result1 = linear_svm(X1, y1)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X_task6 = data_100.iloc[:, :-1].to_numpy()\n",
    "y_task6 = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "result = linear_svm(X_task6, y_task6.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_svm(X: np.ndarray, y: np.ndarray):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "    gaussian_kernel_classifier = svm.SVC(kernel='rbf')\n",
    "    gaussian_kernel_classifier.fit(X_train, y_train)\n",
    "\n",
    "    gaussian_kernel_classifier_predictions = gaussian_kernel_classifier.predict(X_test)\n",
    "\n",
    "    return gaussian_kernel_classifier_predictions, gaussian_kernel_classifier.score(X_test, y_test) * 100\n",
    "\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, -1], [111.2, 20, -1], [111.3, 10, -1], [111.4, 10, -1], [111.5, 10, -1], [211.6, 80, 1],\n",
    "        [111.4, 10, -1], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "result1 = gaussian_kernel_svm(X1, y1)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X_task6 = data_100.iloc[:, :-1].to_numpy()\n",
    "y_task6 = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "result = gaussian_kernel_svm(X_task6, y_task6.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right snippets of code into the corresponding box that says \"Your answer,\"and click \"Save.\" After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click \"Finalize submission.\" Note that once your submission is finalized, it is considered to be submitted for grading and cannot be changed. If you need to undo this action, you will have to email your assigned tutor for help. Please do not finalize your submission until you are sure that you want to submit your solutions for grading.\n",
    "\n",
    "*Have fun and enjoy coding.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
